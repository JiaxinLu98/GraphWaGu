export declare const apply_forces = "struct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct Forces {\n    forces : array<f32>,\n};\nstruct Batch {\n    batch_id : u32,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\nstruct Range {\n    x_min : atomic<i32>,\n    x_max : atomic<i32>,\n    y_min : atomic<i32>,\n    y_max : atomic<i32>,\n};\n@group(0) @binding(0) var<storage, read_write> nodes : Nodes;\n@group(0) @binding(1) var<storage, read_write> forces : Forces;\n// @group(0) @binding(2) var<uniform> batch : Batch;\n@group(0) @binding(2) var<uniform> uniforms : Uniforms;\n@group(0) @binding(3) var<storage, read_write> bounding : Range;\n\n@compute @workgroup_size(128, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    if (global_id.x >= uniforms.nodes_length) {\n        return;\n    }\n    var high : f32 = 8.0;\n    var low : f32 = -7.0;\n    var batch_index : u32 = global_id.x;\n    for (var iter = 0u; iter < 2u; iter = iter + 1u) {\n        // nodes.nodes[batch_index].x = nodes.nodes[batch_index].x + forces.forces[batch_index * 2u];\n        // nodes.nodes[batch_index].y = nodes.nodes[batch_index].y + forces.forces[batch_index * 2u + 1u]; \n        if (forces.forces[batch_index * 2u] > uniforms.cooling_factor) {\n            // atomicStore(&bounding.y_max, i32(batch_index));\n            forces.forces[batch_index * 2u] = 0.0;    \n        }\n        if (forces.forces[batch_index * 2u + 1u] > uniforms.cooling_factor) {\n            // atomicStore(&bounding.y_min, i32(batch_index));\n            forces.forces[batch_index * 2u + 1u] = 0.0;    \n        }\n        // var x : f32 = min(high, max(low, nodes.nodes[batch_index].x + forces.forces[batch_index * 2u]));\n        // var y : f32 = min(high, max(low, nodes.nodes[batch_index].y + forces.forces[batch_index * 2u + 1u]));\n        var x : f32 = nodes.nodes[batch_index].x + forces.forces[batch_index * 2u];\n        var y : f32 = nodes.nodes[batch_index].y + forces.forces[batch_index * 2u + 1u];\n\n        // var centering : vec2<f32> = normalize(vec2<f32>(0.5, 0.5) - vec2<f32>(x, y));\n        // var dist : f32 = distance(vec2<f32>(0.5, 0.5), vec2<f32>(x, y));\n        // x = x + centering.x * (0.1 * uniforms.cooling_factor * dist);\n        // y = y + centering.y * (0.1 * uniforms.cooling_factor * dist);\n        // Randomize position slightly to prevent exact duplicates after clamping\n        // if (x == high) {\n        //     x = x - f32(batch_index) / 500000.0; \n        // } \n        // if (y == high) {\n        //     y = y - f32(batch_index) / 500000.0; \n        // }\n        // if (x == low) {\n        //     x = x + f32(batch_index) / 500000.0; \n        // }\n        // if (y == low) {\n        //     y = y + f32(batch_index) / 500000.0; \n        // }\n        nodes.nodes[batch_index].x = x;\n        nodes.nodes[batch_index].y = y;\n        forces.forces[batch_index * 2u] = 0.0;\n        forces.forces[batch_index * 2u + 1u] = 0.0;\n        atomicMin(&bounding.x_min, i32(floor(x * 1000.0)));\n        atomicMax(&bounding.x_max, i32(ceil(x * 1000.0)));\n        atomicMin(&bounding.y_min, i32(floor(y * 1000.0)));\n        atomicMax(&bounding.y_max, i32(ceil(y * 1000.0)));\n\n\n        // var test : f32 = forces.forces[0]; \n        // var test2 : f32 = nodes.nodes[0].x;\n        batch_index = batch_index + (uniforms.nodes_length / 2u);\n    }\n}\n";
export declare const compute_attractive_new = "struct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct Forces {\n    forces : array<f32>,\n};\nstruct UintArray {\n    a : array<u32>,\n};\nstruct EdgeInfo {\n    source_start : u32,\n    source_degree : u32,\n    dest_start : u32,\n    dest_degree : u32,\n}\nstruct EdgeInfoArray {\n    a : array<EdgeInfo>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\n\n@group(0) @binding(0) var<storage, read_write> edge_info : EdgeInfoArray;\n@group(0) @binding(1) var<storage, read> source_list : UintArray;\n@group(0) @binding(2) var<storage, read> dest_list : UintArray;\n@group(0) @binding(3) var<storage, read_write> forces : Forces;\n@group(0) @binding(4) var<storage, read> nodes : Nodes;\n@group(0) @binding(5) var<uniform> uniforms : Uniforms;\n\n@compute @workgroup_size(128, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    if (global_id.x >= uniforms.nodes_length) {\n        return;\n    }\n    let l : f32 = uniforms.ideal_length;\n    var node : Node = nodes.nodes[global_id.x];\n    var a_force : vec2<f32> = vec2<f32>(0.0, 0.0);\n    var info : EdgeInfo = edge_info.a[global_id.x];\n    // Accumulate forces where node is the source\n    for (var i : u32 = info.source_start; i < info.source_start + info.source_degree; i = i + 1u) {\n        var node2 : Node = nodes.nodes[source_list.a[i]];\n        var dist : f32 = distance(vec2<f32>(node.x, node.y), vec2<f32>(node2.x, node2.y));\n        if(dist > 0.0000001) {\n            var dir : vec2<f32> = normalize(vec2<f32>(node2.x, node2.y) - vec2<f32>(node.x, node.y));\n            a_force = a_force + ((dist * dist) / l) * dir;\n        }\n    }\n    // Accumulate forces where node is the dest\n    for (var i : u32 = info.dest_start; i < info.dest_start + info.dest_degree; i = i + 1u) {\n        var node2 : Node = nodes.nodes[dest_list.a[i]];\n        var dist : f32 = distance(vec2<f32>(node.x, node.y), vec2<f32>(node2.x, node2.y));\n        if(dist > 0.0000001) {\n            var dir : vec2<f32> = normalize(vec2<f32>(node2.x, node2.y) - vec2<f32>(node.x, node.y));\n            a_force = a_force + ((dist * dist) / l) * dir;\n        }\n    }\n    forces.forces[global_id.x * 2u] = a_force.x;\n    forces.forces[global_id.x * 2u + 1u] = a_force.y;\n}";
export declare const compute_attract_forces = "struct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct Edges {\n    edges : array<u32>,\n};\nstruct Forces {\n    forces : array<f32>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\n\n@group(0) @binding(0) var<storage, read> nodes : Nodes;\n@group(0) @binding(1) var<storage, read> edges : Edges;\n@group(0) @binding(2) var<storage, read_write> forces : Forces;\n@group(0) @binding(3) var<uniform> uniforms : Uniforms;\n\n@compute @workgroup_size(1, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    // let i : u32 = global_id.x;\n    let l : f32 = uniforms.ideal_length;\n    for (var i : u32 = 0u; i < uniforms.edges_length; i = i + 2u) {\n        var a_force : vec2<f32> = vec2<f32>(0.0, 0.0);\n        var node : Node = nodes.nodes[edges.edges[i]];\n        var node2 : Node = nodes.nodes[edges.edges[i + 1u]];\n        var dist : f32 = distance(vec2<f32>(node.x, node.y), vec2<f32>(node2.x, node2.y));\n        if(dist > 0.0) {\n            var dir : vec2<f32> = normalize(vec2<f32>(node2.x, node2.y) - vec2<f32>(node.x, node.y));\n            a_force = ((dist * dist) / l) * dir;\n            forces.forces[edges.edges[i] * 2u] = forces.forces[edges.edges[i] * 2u] + a_force.x;\n            forces.forces[edges.edges[i] * 2u + 1u] = forces.forces[edges.edges[i] * 2u + 1u] + a_force.y;\n            forces.forces[edges.edges[i + 1u] * 2u] = forces.forces[edges.edges[i + 1u] * 2u] - a_force.x;\n            forces.forces[edges.edges[i + 1u] * 2u + 1u] = forces.forces[edges.edges[i + 1u] * 2u + 1u] - a_force.y;\n        }\n    }\n}";
export declare const compute_forces = "struct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct Edges {\n    edges : array<u32>,\n};\nstruct Forces {\n    forces : array<f32>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\n\n@group(0) @binding(0) var<storage, read> nodes : Nodes;\n@group(0) @binding(1) var<storage, read> adjmat : Edges;\n@group(0) @binding(2) var<storage, read_write> forces : Forces;\n@group(0) @binding(3) var<uniform> uniforms : Uniforms;\n\nfn get_bit_selector(bit_index : u32) -> u32 {\n    return 1u << bit_index;\n}\n\nfn get_nth_bit(packed : u32, bit_index : u32) -> u32 {\n    return packed & get_bit_selector(bit_index);\n}\n\n@compute @workgroup_size(1, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    let l : f32 = uniforms.ideal_length;\n    let node : Node = nodes.nodes[global_id.x];\n    var r_force : vec2<f32> = vec2<f32>(0.0, 0.0);\n    var a_force : vec2<f32> = vec2<f32>(0.0, 0.0);\n    for (var i : u32 = 0u; i < uniforms.nodes_length; i = i + 1u) {\n        if (i == global_id.x) {\n            continue;\n        }\n        var node2 : Node = nodes.nodes[i];\n        var dist : f32 = distance(vec2<f32>(node.x, node.y), vec2<f32>(node2.x, node2.y));\n        if (dist > 0.0){\n            if (get_nth_bit(adjmat.edges[(i * uniforms.nodes_length + global_id.x) / 32u], (i * uniforms.nodes_length + global_id.x) % 32u) != 0u) {\n                var dir : vec2<f32> = normalize(vec2<f32>(node2.x, node2.y) - vec2<f32>(node.x, node.y));\n                a_force = a_force + ((dist * dist) / l) * dir;\n            } else {\n                var dir : vec2<f32> = normalize(vec2<f32>(node.x, node.y) - vec2<f32>(node2.x, node2.y));\n                r_force = r_force + ((l * l) / dist) * dir;\n            }\n        }\n    }\n    var force : vec2<f32> = (a_force + r_force);\n    var localForceMag: f32 = length(force); \n    if (localForceMag>0.000000001) {\n        force = normalize(force) * min(uniforms.cooling_factor, length(force));\n    }\n    else{\n        force.x = 0.0;\n        force.y = 0.0;\n    }\n    forces.forces[global_id.x * 2u] = force.x;\n    forces.forces[global_id.x * 2u + 1u] = force.y;\n}\n";
export declare const compute_forcesBH = "const cluster_size = CHANGEMEu;\nstruct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Edges {\n    edges : array<u32>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\nstruct TreeInfo {\n    step : u32,\n    max_index : u32,\n    theta : f32,\n};\nstruct Range {\n    x_min : i32,\n    x_max : i32,\n    y_min : i32,\n    y_max : i32,\n};\nstruct TreeNode {\n    // x, y, width, height\n    boundary : vec4<f32>,\n    CoM : vec2<f32>,\n    mass : f32,\n    test : u32,\n    code : u32,\n    level : u32,\n    test2: u32,\n    test3: u32,\n    pointers : array<u32, cluster_size>,\n};\n\n@group(0) @binding(0) var<storage, read> nodes : array<Node>;\n@group(0) @binding(1) var<storage, read_write> forces : array<f32>;\n@group(0) @binding(2) var<uniform> uniforms : Uniforms;\n@group(0) @binding(3) var<uniform> tree_info : TreeInfo;\n@group(0) @binding(4) var<storage, read> tree : array<TreeNode>;\n\n@compute @workgroup_size(128, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    var stack = array<u32, 64>();\n    let l : f32 = uniforms.ideal_length;\n    var index : u32 = global_id.x;\n    if (index >= uniforms.nodes_length) {\n        return;\n    }\n    let node = nodes[index];\n    var theta : f32 = tree_info.theta;\n    var r_force : vec2<f32> = vec2<f32>(0.0, 0.0);\n    var a_force : vec2<f32> = vec2<f32>(forces[index * 2u], forces[index * 2u + 1u]);\n    var tree_idx : u32 = tree_info.max_index;\n    var counter : u32 = 0u;\n    var out : u32 = 0u;\n    loop {\n        out = out + 1u;\n        // if (out == 1000u) {\n        //     break;\n        // }\n        var tree_node = tree[tree_idx];\n        let dist : f32 = distance(vec2<f32>(node.x, node.y), tree_node.CoM);\n        let s : f32 = 2.0 * tree_node.boundary.w;\n        if (theta > s / dist) {\n            var dir : vec2<f32> = normalize(vec2<f32>(node.x, node.y) - tree_node.CoM);\n            r_force = r_force + tree_node.mass * ((l * l) / dist) * dir;\n        } else {\n            for (var i : u32 = 0u; i < cluster_size; i = i + 1u) {\n                let child : u32 = tree_node.pointers[i];\n                if (child == 0 || tree[child].mass < 1.0) {\n                    continue;\n                } else {\n                    if (tree[child].mass > 1.0) {\n                        stack[counter] = child;\n                        counter = counter + 1u;\n                    } else {\n                        let dist : f32 = distance(vec2<f32>(node.x, node.y), tree[child].CoM);\n                        if (dist > 0.0) {\n                            var dir : vec2<f32> = normalize(vec2<f32>(node.x, node.y) - tree[child].CoM);\n                            r_force = r_force + ((l * l) / dist) * dir;\n                        }\n                    }\n                }\n            }\n        }\n        counter--;\n        if (counter < 0u) {\n            break;\n        }\n        tree_idx = stack[counter];\n        if (tree_idx == 0u) {\n            break;\n        } \n    }\n    var force : vec2<f32> = (a_force + r_force);\n    var localForceMag: f32 = length(force); \n    if (localForceMag>0.000000001) {\n        force = normalize(force) * min(uniforms.cooling_factor, length(force));\n    }\n    else{\n        force.x = 0.0;\n        force.y = 0.0;\n    }\n    if (force.x > uniforms.cooling_factor) {\n        force.x = 0.0;\n    }\n    if (force.y > uniforms.cooling_factor) {\n        force.y = 0.0;\n    }\n    forces[index * 2u] = force.x;\n    forces[index * 2u + 1u] = force.y;\n}\n";
export declare const create_adjacency_matrix = "struct Edges {\n    edges : array<u32>,\n};\nstruct BoolArray {\n    matrix : array<u32>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\nstruct IntArray {\n    matrix : array<i32>,\n};\n\n@group(0) @binding(0) var<storage, read> edges : Edges;\n@group(0) @binding(1) var<storage, read_write> adjmat : BoolArray;\n@group(0) @binding(2) var<uniform> uniforms : Uniforms;\n@group(0) @binding(3) var<storage, read_write> laplacian : IntArray;\n\nfn get_bit_selector(bit_index : u32) -> u32 {\n    return 1u << bit_index;\n}\n\nfn set_nth_bit(packed : u32, bit_index : u32) -> u32{\n    return packed | get_bit_selector(bit_index);\n}\n\n@compute @workgroup_size(1, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    for (var i : u32 = 0u; i < uniforms.edges_length; i = i + 2u) {\n        var source : u32 = edges.edges[i];\n        var dest : u32 = edges.edges[i + 1u];\n        adjmat.matrix[(source * uniforms.nodes_length + dest) / 32u] = set_nth_bit(adjmat.matrix[(source * uniforms.nodes_length + dest) / 32u], (source * uniforms.nodes_length + dest) % 32u);\n        adjmat.matrix[(dest * uniforms.nodes_length + source) / 32u] = set_nth_bit(adjmat.matrix[(dest * uniforms.nodes_length + source) / 32u], (dest * uniforms.nodes_length + source) % 32u);\n        // if (laplacian.matrix[source * uniforms.nodes_length + dest] != -1 && source != dest) {\n        //     laplacian.matrix[source * uniforms.nodes_length + dest] = -1;\n        //     laplacian.matrix[dest * uniforms.nodes_length + source] = -1;\n        //     laplacian.matrix[source * uniforms.nodes_length + source] = laplacian.matrix[source * uniforms.nodes_length + source] + 1;\n        //     laplacian.matrix[dest * uniforms.nodes_length + dest] = laplacian.matrix[dest * uniforms.nodes_length + dest] + 1;\n        // }\n    } \n}\n";
export declare const create_sourcelist = "struct Edges {\n    edges : array<u32>,\n};\nstruct UintArray {\n    a : array<u32>,\n};\nstruct EdgeInfo {\n    source_start : u32,\n    source_degree : u32,\n    dest_start : u32,\n    dest_degree : u32,\n}\nstruct EdgeInfoArray {\n    a : array<EdgeInfo>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\n\n@group(0) @binding(0) var<storage, read_write> edges : Edges;\n@group(0) @binding(1) var<storage, read_write> edge_info : EdgeInfoArray;\n@group(0) @binding(2) var<storage, read_write> source_list : UintArray;\n@group(0) @binding(3) var<uniform> uniforms : Uniforms;\n\n@compute @workgroup_size(1, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    var counter : u32 = 0u;\n    var source : u32 = 0u;\n    // expects edges to be sorted by source id\n    for (var i : u32 = 0u; i < uniforms.edges_length; i = i + 2u) {\n        var new_source : u32 = edges.edges[i];\n        var dest : u32 = edges.edges[i + 1u];\n        edge_info.a[new_source].source_degree = edge_info.a[new_source].source_degree + 1u;\n        source_list.a[counter] = dest;\n        if (new_source != source || i == 0u) {\n            edge_info.a[new_source].source_start = counter;\n        }\n        counter = counter + 1u;\n        source = new_source;\n    }\n}";
export declare const create_targetlist = "struct Edges {\n    edges : array<u32>,\n};\nstruct UintArray {\n    a : array<u32>,\n};\nstruct EdgeInfo {\n    source_start : u32,\n    source_degree : u32,\n    dest_start : u32,\n    dest_degree : u32,\n}\nstruct EdgeInfoArray {\n    a : array<EdgeInfo>,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\n\n@group(0) @binding(0) var<storage, read_write> edges : Edges;\n@group(0) @binding(1) var<storage, read_write> edge_info : EdgeInfoArray;\n@group(0) @binding(2) var<storage, read_write> dest_list : UintArray;\n@group(0) @binding(3) var<uniform> uniforms : Uniforms;\n\n@compute @workgroup_size(1, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    var counter : u32 = 0u;\n    var dest : u32 = 0u;\n    // expects edges to be sorted by dest id\n    for (var i : u32 = 0u; i < uniforms.edges_length; i = i + 2u) {\n        var source : u32 = edges.edges[i];\n        var new_dest : u32 = edges.edges[i + 1u];\n        edge_info.a[new_dest].dest_degree = edge_info.a[new_dest].dest_degree + 1u;\n        dest_list.a[counter] = source;\n        if (new_dest != dest || i == 0u) {\n            edge_info.a[new_dest].dest_start = counter;\n        }\n        counter = counter + 1u;\n        dest = new_dest;\n    }\n}";
export declare const create_tree = "const cluster_size = CHANGEMEu;\nstruct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\nstruct TreeInfo {\n    step : u32,\n    max_index : u32,\n    theta: f32,\n    cluster_size: u32,\n};\nstruct Range {\n    x_min : i32,\n    x_max : i32,\n    y_min : i32,\n    y_max : i32,\n};\nstruct TreeNode {\n    // x, y, width, height\n    boundary : vec4<f32>,\n    CoM : vec2<f32>,\n    mass : f32,\n    test : u32,\n    code : u32,\n    level : u32,\n    test2: u32,\n    test3: u32,\n    pointers : array<u32, cluster_size>,\n};\n\n@group(0) @binding(0) var<storage, read> indices : array<u32>;\n@group(0) @binding(1) var<uniform> uniforms : Uniforms;\n@group(0) @binding(2) var<uniform> tree_info : TreeInfo;\n@group(0) @binding(3) var<storage, read_write> bounding : Range;\n@group(0) @binding(4) var<storage, read_write> tree : array<TreeNode>;\n\n// Find the level above where two Morton codes first disagree\nfn find_morton_split_level(morton1: u32, morton2: u32) -> u32 {\n    // XOR the Morton codes to find differing bits\n    let diff = morton1 ^ morton2;\n    \n    // If codes are identical, return 16\n    if (diff == 0u) {\n        return 16u;\n    }\n    \n    // Find position of highest different bit\n    var highest_diff_bit = 31u;\n    var temp = diff;\n    \n    // Count leading zeros\n    while ((temp & 0x80000000u) == 0u) {\n        temp = temp << 1u;\n        highest_diff_bit = highest_diff_bit - 1u;\n    }\n    \n    // Convert bit position to level\n    let level = 16u - (highest_diff_bit + 2u) / 2u;\n    return level;\n}\n\n@compute @workgroup_size(128, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    let x_min = f32(bounding.x_min) / 1000.0;\n    let x_max = f32(bounding.x_max) / 1000.0;\n    let y_min = f32(bounding.y_min) / 1000.0;\n    let y_max = f32(bounding.y_max) / 1000.0;\n    let step = tree_info.step;\n    var idx = global_id.x * cluster_size;\n    var start = f32(uniforms.nodes_length);\n    var end = uniforms.nodes_length;\n    for (var i = 0u; i < step; i++) {\n        idx += u32(start);\n        start = ceil(start / f32(cluster_size));\n        end += u32(start);\n    }\n    if (idx >= end) {\n        return;\n    }\n    var pointers = array<u32, cluster_size>();\n    if (step == 0u) {\n        for (var i = 0u; i < cluster_size; i++) {\n            if (idx + i >= end) {\n                pointers[i] = 0;\n            } else {\n                pointers[i] = indices[idx + i] + 1;\n            }\n        }\n    } else {\n        for (var i = 0u; i < cluster_size; i++) {\n             if (idx + i >= end) {\n                pointers[i] = 0;\n            } else {\n                pointers[i] = idx + i + 1;\n            }\n        }\n    }\n    var node = tree[pointers[0]];\n    var code = node.code;\n    var level = node.level;\n    var mass = node.mass;\n    var CoM = node.CoM;\n    for (var i = 1u; i < cluster_size; i++) {\n        if (idx + i >= end) {\n            break;\n        }\n        node = tree[pointers[i]];\n        level = min(find_morton_split_level(code, node.code), min(level, node.level));\n        CoM = (mass * CoM + node.mass * node.CoM) / (mass + node.mass);\n        mass = mass + node.mass;\n    }\n    tree[end + global_id.x + 1] = TreeNode(\n        vec4<f32>(0.0, 0.0, (1.0 / f32(1u << level)) * (x_max - x_min), (1.0 / f32(1u << level)) * (y_max - y_min)),\n        CoM,\n        mass, \n        0u, \n        code, level, 0u, 0u,\n        pointers,\n    );\n    //  PROBLEM WITH POINTERS ARRAY\n    // let node1 = tree[pointers[0]];\n    // let node2 = tree[pointers[1]];\n    // let node3 = tree[pointers[2]];\n    // let node4 = tree[pointers[3]];\n    // let morton1 = node1.code;\n    // let morton2 = node2.code;\n    // let morton3 = node3.code;\n    // let morton4 = node4.code;\n    // if (idx == end - 1) {\n    //     // Just write the node out without merging with anything\n    //     tree[end + global_id.x + 1] = node1;\n    //     return;\n    // }\n    // if (idx == end - 2) {\n    //     let level = min(find_morton_split_level(morton1, morton2), min(node1.level, node2.level));\n    //     tree[end + global_id.x + 1] = TreeNode(\n    //         vec4<f32>(0.0, 0.0, 1.0 / f32(1u << level), 1.0 / f32(1u << level)),\n    //         (node1.mass * node1.CoM + node2.mass * node2.CoM) / (node1.mass + node2.mass),\n    //         node1.mass + node2.mass, \n    //         morton2, \n    //         pointers,\n    //         morton1, level\n    //     );\n    //     return;\n    // }\n    // if (idx == end - 3) {\n    //     let level = min(min(find_morton_split_level(morton3, morton2), min(find_morton_split_level(morton1, morton2), min(node1.level, node2.level))), node3.level);\n    //     tree[end + global_id.x + 1] = TreeNode(\n    //         vec4<f32>(0.0, 0.0, 1.0 / f32(1u << level), 1.0 / f32(1u << level)),\n    //         (node1.mass * node1.CoM + node2.mass * node2.CoM + node3.mass * node3.CoM) / (node1.mass + node2.mass + node3.mass),\n    //         node1.mass + node2.mass + node3.mass, \n    //         morton2, \n    //         pointers,\n    //         morton1, level\n    //     );\n    //     return;\n    // }\n    // let level12 = min(find_morton_split_level(morton1, morton2), min(node1.level, node2.level));\n    // let level34 = min(find_morton_split_level(morton3, morton4), min(node3.level, node4.level));\n    // let level = min(find_morton_split_level(morton2, morton3), min(level12, level34));\n    // tree[end + global_id.x + 1] = TreeNode(\n    //     vec4<f32>(0.0, 0.0, 1.0 / f32(1u << level), 1.0 / f32(1u << level)),\n    //     (node1.mass * node1.CoM + node2.mass * node2.CoM + node3.mass * node3.CoM + node4.mass * node4.CoM) / (node1.mass + node2.mass + node3.mass + node4.mass),\n    //     node1.mass + node2.mass + node3.mass + node4.mass, \n    //     morton2, \n    //     pointers,\n    //     morton1, level\n    // );\n}\n";
export declare const edge_frag = "@fragment\nfn main()->@location(0) vec4<f32>{\n    return vec4<f32>(0.0, 0.0, 0.0, 0.1);\n}";
export declare const edge_vert = "//this builtin(position) clip_position tells that clip_position is the value we want to use for our vertex position or clip position\n//it's not needed to create a struct, we could just do [[builtin(position)]] clipPosition\nstruct VertexOutput{\n    @builtin(position) clip_position: vec4<f32>,\n};\nstruct Uniforms {\n  view_box : vec4<f32>,\n};\nstruct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct Edges {\n    edges : array<u32>,\n};\n\n@group(0) @binding(0) var<uniform> uniforms : Uniforms;\n@group(0) @binding(1) var<storage, read> nodes : Nodes;\n@group(0) @binding(2) var<storage, read> edges : Edges;\n@vertex\nfn main(@builtin(instance_index) index : u32, @location(0) position: vec2<f32>)-> VertexOutput {\n    var out : VertexOutput;\n    var node : Node = nodes.nodes[edges.edges[2u * index + u32(position.x)]];\n    var inv_zoom : f32 = uniforms.view_box.z - uniforms.view_box.x;\n    var expected_x : f32 = 0.5 * (1.0 - inv_zoom); \n    var expected_y : f32 = 0.5 * (1.0 - inv_zoom);\n    // view_box expected to be between 0 and 1, panning need to be doubled as clip space is (-1, 1)\n    var x : f32 = ((2.0 * node.x - 1.0) - 2.0 * (uniforms.view_box.x - expected_x)) / inv_zoom;\n    var y : f32 = ((2.0 * node.y - 1.0) - 2.0 * (uniforms.view_box.y - expected_y)) / inv_zoom;\n    out.clip_position = vec4<f32>(x, y, 0.0, 1.0);\n    return out;\n}";
export declare const morton_codes = "const cluster_size = CHANGEMEu;\nstruct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Uniforms {\n    nodes_length : u32,\n    edges_length : u32,\n    cooling_factor : f32,\n    ideal_length : f32,\n};\nstruct Range {\n    x_min : i32,\n    x_max : i32,\n    y_min : i32,\n    y_max : i32,\n};\nstruct TreeNode {\n    // x, y, width, height\n    boundary : vec4<f32>,\n    CoM : vec2<f32>,\n    mass : f32,\n    test : u32,\n    code : u32,\n    level : u32,\n    test2: u32,\n    test3: u32,\n    pointers : array<u32, cluster_size>,\n};\n\n@group(0) @binding(0) var<storage, read> nodes : array<Node>;\n@group(0) @binding(1) var<storage, read_write> morton_codes : array<u32>;\n@group(0) @binding(2) var<uniform> uniforms : Uniforms;\n@group(0) @binding(3) var<storage, read_write> bounding : Range;\n@group(0) @binding(4) var<storage, read_write> morton_indices : array<u32>;\n@group(0) @binding(5) var<storage, read_write> tree : array<TreeNode>;\n\n// Spreads bits by inserting 0s between each bit\nfn spread_bits(x: u32) -> u32 {\n    var x_mut = x & 0x0000FFFF;  // Mask to ensure we only use lower 16 bits\n    x_mut = (x_mut | (x_mut << 8)) & 0x00FF00FF;\n    x_mut = (x_mut | (x_mut << 4)) & 0x0F0F0F0F;\n    x_mut = (x_mut | (x_mut << 2)) & 0x33333333;\n    x_mut = (x_mut | (x_mut << 1)) & 0x55555555;\n    return x_mut;\n}\n\n// Converts float in [0,1] to fixed-point integer\n// TODO: precision lost here\nfn float_to_fixed(f: f32) -> u32 {\n    return u32(f * 65535.0);  // 2^16 - 1\n}\n\n// Convert morton code to quadrant boundaries\nfn morton_to_rectangle(morton: u32, level: u32) -> vec4<f32> {    \n    // Initialize normalized coordinates\n    var x = 0.0;\n    var y = 0.0;\n    var size = 1.0;\n    \n    // Process each pair of bits from most significant to least\n    for(var i = 0u; i < level; i++) {\n        size *= 0.5; // Each level divides size by 2\n        let shift = (15u - i) * 2u;\n        let bits = (morton >> shift) & 3u; // Get pair of bits\n        \n        // Update position based on quadrant\n        switch bits {\n            case 0u: { // 00: bottom left\n                // Position stays the same\n            }\n            case 1u: { // 01: bottom right\n                x += size;\n            }\n            case 2u: { // 10: top left\n                y += size;\n            }\n            case 3u: { // 11: top right\n                x += size;\n                y += size;\n            }\n            default: {}\n        }\n    }\n    \n    // Convert from normalized coordinates to world space\n    let x_min = f32(bounding.x_min) / 1000.0;\n    let x_max = f32(bounding.x_max) / 1000.0;\n    let y_min = f32(bounding.y_min) / 1000.0;\n    let y_max = f32(bounding.y_max) / 1000.0;\n    \n    let world_x = x * (x_max - x_min) + x_min;\n    let world_y = y * (y_max - y_min) + y_min;\n    let world_w = size * (x_max - x_min);\n    let world_h = size * (y_max - y_min);\n    \n    return vec4<f32>(world_x, world_y, world_w, world_h);\n}\n\nfn rotate_bits(n: u32, rx: u32, ry: u32, order: u32) -> u32 {\n    if (ry == 0u) {\n        if (rx == 1u) {\n            // Reflect about y=x\n            let mask = (1u << order) - 1u;\n            return mask - n;\n        }\n    }\n    return n;\n}\n\nfn hilbert_xy_to_d(x_in: u32, y_in: u32) -> u32 {\n    var d: u32 = 0u;\n    var x: u32 = x_in;\n    var y: u32 = y_in;\n    var rx: u32;\n    var ry: u32;\n    \n    // Process 16 bits of input coordinates\n    for(var i: u32 = 0u; i < 16u; i += 1u) {\n        let s = 15u - i;\n        \n        // Extract current bit of x and y from highest positions\n        rx = (x >> 15u) & 1u;\n        ry = (y >> 15u) & 1u;\n        \n        // Add position to result\n        d |= ((3u * rx) ^ ry) << (2u * s);\n        \n        // Rotate coordinates if needed for next iteration\n        if (ry == 0u) {\n            if (rx == 1u) {\n                // Reflect about y=x\n                x = (1u << 16u) - 1u - x;\n                y = (1u << 16u) - 1u - y;\n            }\n            // Swap x and y\n            let t = x;\n            x = y;\n            y = t;\n        }\n        \n        // Shift coordinates for next iteration\n        x = (x << 1u) & 0xFFFFu;\n        y = (y << 1u) & 0xFFFFu;\n    }\n    \n    return d;\n}\n\n@compute @workgroup_size(128, 1, 1)\nfn main(@builtin(global_invocation_id) global_id : vec3<u32>) {\n    let idx = global_id.x;\n    if (idx >= uniforms.nodes_length) {\n        return;\n    }\n    let node = nodes[idx];\n    \n    // Convert floats to fixed-point\n    let x_min = f32(bounding.x_min) / 1000.0;\n    let x_max = f32(bounding.x_max) / 1000.0;\n    let y_min = f32(bounding.y_min) / 1000.0;\n    let y_max = f32(bounding.y_max) / 1000.0;\n    let x_fixed = float_to_fixed((node.x - x_min) / (x_max - x_min));\n    let y_fixed = float_to_fixed((node.y - y_min) / (y_max - y_min));\n    \n    // Compute Morton code by interleaving bits\n    let morton = spread_bits(x_fixed) | (spread_bits(y_fixed) << 1);\n    let hilbert = hilbert_xy_to_d(x_fixed, y_fixed);\n    let code = hilbert;\n    \n    morton_codes[idx] = code;\n    // morton_codes[idx] = morton;\n    morton_indices[idx] = idx;\n    // tree[idx + 1u] = TreeNode(\n    //     morton_to_rectangle(morton, 16),\n    //     vec2<f32>(node.x, node.y),\n    //     1.0, 0.0, vec4<u32>(0u),\n    //     morton, 16u\n    // );\n    tree[idx + 1u] = TreeNode(\n        vec4<f32>(0.0, 0.0, (1.0 / f32(1u << 16u)) * (x_max - x_min), (1.0 / f32(1u << 16u)) * (y_max - y_min)),\n        vec2<f32>(node.x, node.y),\n        1.0, 0u,\n        code, 16u, 0u, 0u,\n        array<u32, cluster_size>()\n    );\n}\n";
export declare const node_frag = "fn sigmoid(x: f32) -> f32 {\n    return 1.0 / (1.0 + exp(-1.0 * x));\n}\n\n@fragment\nfn main(@location(0) position: vec2<f32>, @location(1) @interpolate(flat) center: vec2<f32>, @location(2) color: vec3<f32>) -> @location(0) vec4<f32> {\n    if (distance(position, center) > 0.002) {\n        discard;\n    }\n    return vec4<f32>(color.x, color.y, color.z, 1.0);\n}\n";
export declare const node_vert = "struct Node {\n    value : f32,\n    x : f32,\n    y : f32,\n    size : f32,\n};\nstruct Nodes {\n    nodes : array<Node>,\n};\nstruct VertexOutput {\n    @builtin(position) Position : vec4<f32>,\n    @location(0) position: vec2<f32>,\n    @location(1) @interpolate(flat) center : vec2<f32>,\n    @location(2) color: vec3<f32>,\n};\nstruct Uniforms {\n  view_box : vec4<f32>,\n};\nstruct Edges {\n    edges : array<u32>,\n};\n\n@group(0) @binding(0) var<uniform> uniforms : Uniforms;\n@group(0) @binding(1) var<storage, read> nodes : Nodes;\n@group(0) @binding(2) var<storage, read> morton_codes : array<u32>;\n\nfn u32_to_color(value: u32) -> vec3<f32> {\n    // First convert u32 to f32 in [0,1] range\n    // We need to be careful about precision here\n    // Break the u32 into two parts to maintain precision\n    let upper = f32(value >> 16u);\n    let lower = f32(value & 0xFFFFu);\n    \n    // Combine the parts with appropriate scaling\n    let normalized = (upper * 65536.0 + lower) / 4294967295.0;\n    \n    // Define the color gradient\n    // Here we'll use a simple RGB gradient: blue -> cyan -> green -> yellow -> red\n    let positions = array<f32, 5>(0.0, 0.25, 0.5, 0.75, 1.0);\n    let colors = array<vec3<f32>, 5>(\n        vec3<f32>(0.0, 0.0, 1.0),  // Blue\n        vec3<f32>(0.0, 1.0, 1.0),  // Cyan\n        vec3<f32>(0.0, 1.0, 0.0),  // Green\n        vec3<f32>(1.0, 1.0, 0.0),  // Yellow\n        vec3<f32>(1.0, 0.0, 0.0)   // Red\n    );\n    \n    // Find the segment\n    var i = 0;\n    while i < 4 && normalized > positions[i + 1] {\n        i = i + 1;\n    }\n    \n    // Calculate interpolation factor\n    let t = (normalized - positions[i]) / (positions[i + 1] - positions[i]);\n    \n    // Interpolate between colors\n    let color = mix(colors[i], colors[i + 1], t);\n    \n    return color;\n}\n\n@vertex\nfn main(@builtin(instance_index) index : u32, @location(0) position : vec2<f32>)\n     -> VertexOutput {\n    var node_center : vec2<f32> = 2.0 * vec2<f32>(nodes.nodes[index].x, nodes.nodes[index].y) - vec2<f32>(1.0);\n    var translation : vec2<f32> = position * 0.01;\n    var out_position : vec2<f32> = node_center + translation;\n    var output : VertexOutput;\n    var inv_zoom : f32 = uniforms.view_box.z - uniforms.view_box.x;\n    var expected_x : f32 = 0.5 * (1.0 - inv_zoom); \n    var expected_y : f32 = 0.5 * (1.0 - inv_zoom);\n    // view_box expected to be between 0 and 1, panning need to be doubled as clip space is (-1, 1)\n    var x : f32 = (out_position.x - 2.0 * (uniforms.view_box.x - expected_x)) / inv_zoom;\n    var y : f32 = (out_position.y - 2.0 * (uniforms.view_box.y - expected_y)) / inv_zoom;\n    output.Position = vec4<f32>(x, y, 0.0, 1.0);\n    output.position = out_position;\n    // flat interpolated position will give bottom right corner, so translate to center\n    output.center = node_center;\n    let test = morton_codes[index];\n    output.color = u32_to_color(test);\n    return output;\n}";
export declare const radix_sort = "// shader implementing gpu radix sort. More information in the beginning of gpu_rs.rs\n// info: \n\n// also the workgroup sizes are added in these prepasses\n// before the pipeline is started the following constant definitionis are prepended to this shadercode\n\n// const histogram_sg_size\n// const histogram_wg_size\n// const rs_radix_log2\n// const rs_radix_size\n// const rs_keyval_size\n// const rs_histogram_block_rows\n// const rs_scatter_block_rows\n\nstruct GeneralInfo {\n    num_keys: u32,\n    padded_size: u32,\n    even_pass: u32,\n    odd_pass: u32,\n};\n\n@group(0) @binding(0)\nvar<storage, read_write> infos: GeneralInfo;\n@group(0) @binding(1)\nvar<storage, read_write> histograms : array<atomic<u32>>;\n@group(0) @binding(2)\nvar<storage, read_write> keys : array<u32>;\n@group(0) @binding(3)\nvar<storage, read_write> keys_b : array<u32>;\n@group(0) @binding(4)\nvar<storage, read_write> payload_a : array<u32>;\n@group(0) @binding(5)\nvar<storage, read_write> payload_b : array<u32>;\n\n// layout of the histograms buffer\n//   +---------------------------------+ <-- 0\n//   | histograms[keyval_size]         |\n//   +---------------------------------+ <-- keyval_size                           * histo_size\n//   | partitions[scatter_blocks_ru-1] |\n//   +---------------------------------+ <-- (keyval_size + scatter_blocks_ru - 1) * histo_size\n//   | workgroup_ids[keyval_size]      |\n//   +---------------------------------+ <-- (keyval_size + scatter_blocks_ru - 1) * histo_size + workgroup_ids_size\n\n// --------------------------------------------------------------------------------------------------------------\n// Filling histograms and keys with default values (also resets the pass infos for odd and even scattering)\n// --------------------------------------------------------------------------------------------------------------\n@compute @workgroup_size({histogram_wg_size})\nfn zero_histograms(@builtin(global_invocation_id) gid: vec3<u32>, @builtin(num_workgroups) nwg: vec3<u32>) {\n    if gid.x == 0u {\n        infos.even_pass = 0u;\n        infos.odd_pass = 1u;    // has to be one, as on the first call to even pass + 1 % 2 is calculated\n    }\n    // here the histograms are set to zero and the partitions are set to 0xfffffffff to avoid sorting problems\n    let scatter_wg_size = histogram_wg_size;\n    let scatter_block_kvs = scatter_wg_size * rs_scatter_block_rows;\n    let scatter_blocks_ru = (infos.num_keys + scatter_block_kvs - 1u) / scatter_block_kvs;\n\n    let histo_size = rs_radix_size;\n    var n = (rs_keyval_size + scatter_blocks_ru - 1u) * histo_size;\n    let b = n;\n    if infos.num_keys < infos.padded_size {\n        n += infos.padded_size - infos.num_keys;\n    }\n\n    let line_size = nwg.x * {histogram_wg_size}u;\n    for (var cur_index = gid.x; cur_index < n; cur_index += line_size){\n        if cur_index >= n {\n            return;\n        }\n            \n        if cur_index  < rs_keyval_size * histo_size {\n            atomicStore(&histograms[cur_index], 0u);\n        }\n        else if cur_index < b {\n            atomicStore(&histograms[cur_index], 0u);\n        }\n        else {\n            keys[infos.num_keys + cur_index - b] = 0xFFFFFFFFu;\n        }\n    }\n}\n\n// --------------------------------------------------------------------------------------------------------------\n// Calculating the histograms\n// --------------------------------------------------------------------------------------------------------------\nvar<workgroup> smem : array<atomic<u32>, rs_radix_size>;\nvar<private> kv : array<u32, rs_histogram_block_rows>;\nfn zero_smem(lid: u32) {\n    if lid < rs_radix_size {\n        atomicStore(&smem[lid], 0u);\n    }\n}\nfn histogram_pass(pass_: u32, lid: u32) {\n    zero_smem(lid);\n    workgroupBarrier();\n\n    for (var j = 0u; j < rs_histogram_block_rows; j++) {\n        let u_val = bitcast<u32>(kv[j]);\n        let digit = extractBits(u_val, pass_ * rs_radix_log2, rs_radix_log2);\n        atomicAdd(&smem[digit], 1u);\n    }\n\n    workgroupBarrier();\n    let histogram_offset = rs_radix_size * pass_ + lid;\n    if lid < rs_radix_size && atomicLoad(&smem[lid]) >= 0u {\n        atomicAdd(&histograms[histogram_offset], atomicLoad(&smem[lid]));\n    }\n}\n\n// the workgrpu_size can be gotten on the cpu by by calling pipeline.get_bind_group_layout(0).unwrap().get_local_workgroup_size();\nfn fill_kv(wid: u32, lid: u32) {\n    let rs_block_keyvals: u32 = rs_histogram_block_rows * histogram_wg_size;\n    let kv_in_offset = wid * rs_block_keyvals + lid;\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_wg_size;\n        kv[i] = keys[pos];\n    }\n}\nfn fill_kv_keys_b(wid: u32, lid: u32) {\n    let rs_block_keyvals: u32 = rs_histogram_block_rows * histogram_wg_size;\n    let kv_in_offset = wid * rs_block_keyvals + lid;\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_wg_size;\n        kv[i] = keys_b[pos];\n    }\n}\n@compute @workgroup_size({histogram_wg_size})\nfn calculate_histogram(@builtin(workgroup_id) wid: vec3<u32>, @builtin(local_invocation_id) lid: vec3<u32>) {\n    // efficient loading of multiple values\n    fill_kv(wid.x, lid.x);\n    \n    // Accumulate and store histograms for passes\n    histogram_pass(3u, lid.x);\n    histogram_pass(2u, lid.x);\n    histogram_pass(1u, lid.x);\n    histogram_pass(0u, lid.x);\n}\n\n// --------------------------------------------------------------------------------------------------------------\n// Prefix sum over histogram\n// --------------------------------------------------------------------------------------------------------------\nfn prefix_reduce_smem(lid: u32) {\n    var offset = 1u;\n    for (var d = rs_radix_size >> 1u; d > 0u; d = d >> 1u) { // sum in place tree\n        workgroupBarrier();\n        if lid < d {\n            let ai = offset * (2u * lid + 1u) - 1u;\n            let bi = offset * (2u * lid + 2u) - 1u;\n            atomicAdd(&smem[bi], atomicLoad(&smem[ai]));\n        }\n        offset = offset << 1u;\n    }\n\n    if lid == 0u {\n        atomicStore(&smem[rs_radix_size - 1u], 0u);\n    } // clear the last element\n\n    for (var d = 1u; d < rs_radix_size; d = d << 1u) {\n        offset = offset >> 1u;\n        workgroupBarrier();\n        if lid < d {\n            let ai = offset * (2u * lid + 1u) - 1u;\n            let bi = offset * (2u * lid + 2u) - 1u;\n\n            let t = atomicLoad(&smem[ai]);\n            atomicStore(&smem[ai], atomicLoad(&smem[bi]));\n            atomicAdd(&smem[bi], t);\n        }\n    }\n}\n@compute @workgroup_size({prefix_wg_size})\nfn prefix_histogram(@builtin(workgroup_id) wid: vec3<u32>, @builtin(local_invocation_id) lid: vec3<u32>) {\n    // the work group  id is the pass, and is inverted in the next line, such that pass 3 is at the first position in the histogram buffer\n    let histogram_base = (rs_keyval_size - 1u - wid.x) * rs_radix_size;\n    let histogram_offset = histogram_base + lid.x;\n    \n    // the following coode now corresponds to the prefix calc code in fuchsia/../shaders/prefix.h\n    // however the implementation is taken from https://www.eecs.umich.edu/courses/eecs570/hw/parprefix.pdf listing 2 (better overview, nw subgroup arithmetic)\n    // this also means that only half the amount of workgroups is spawned (one workgroup calculates for 2 positioons)\n    // the smemory is used from the previous section\n    atomicStore(&smem[lid.x], atomicLoad(&histograms[histogram_offset]));\n    atomicStore(&smem[lid.x + {prefix_wg_size}u], atomicLoad(&histograms[histogram_offset + {prefix_wg_size}u]));\n\n    prefix_reduce_smem(lid.x);\n    workgroupBarrier();\n\n    atomicStore(&histograms[histogram_offset], atomicLoad(&smem[lid.x]));\n    atomicStore(&histograms[histogram_offset + {prefix_wg_size}u], atomicLoad(&smem[lid.x + {prefix_wg_size}u]));\n}\n\n// --------------------------------------------------------------------------------------------------------------\n// Scattering the keys\n// --------------------------------------------------------------------------------------------------------------\n// General note: Only 2 sweeps needed here\nvar<workgroup> scatter_smem: array<u32, rs_mem_dwords>; // note: rs_mem_dwords is caclulated in the beginngin of gpu_rs.rs\n//            | Dwords                                    | Bytes\n//  ----------+-------------------------------------------+--------\n//  Lookback  | 256                                       | 1 KB\n//  Histogram | 256                                       | 1 KB\n//  Prefix    | 4-84                                      | 16-336\n//  Reorder   | RS_WORKGROUP_SIZE * RS_SCATTER_BLOCK_ROWS | 2-8 KB\nfn partitions_base_offset() -> u32 { return rs_keyval_size * rs_radix_size;}\nfn smem_prefix_offset() -> u32 { return rs_radix_size + rs_radix_size;}\nfn rs_prefix_sweep_0(idx: u32) -> u32 { return scatter_smem[smem_prefix_offset() + rs_mem_sweep_0_offset + idx];}\nfn rs_prefix_sweep_1(idx: u32) -> u32 { return scatter_smem[smem_prefix_offset() + rs_mem_sweep_1_offset + idx];}\nfn rs_prefix_sweep_2(idx: u32) -> u32 { return scatter_smem[smem_prefix_offset() + rs_mem_sweep_2_offset + idx];}\nfn rs_prefix_load(lid: u32, idx: u32) -> u32 { return scatter_smem[rs_radix_size + lid + idx];}\nfn rs_prefix_store(lid: u32, idx: u32, val: u32) { scatter_smem[rs_radix_size + lid + idx] = val;}\nfn is_first_local_invocation(lid: u32) -> bool { return lid == 0u;}\n\nfn histogram_load(digit: u32) -> u32 {\n    return atomicLoad(&smem[digit]);\n}\n\nfn histogram_store(digit: u32, count: u32) {\n    atomicStore(&smem[digit], count);\n}\n\n\nconst rs_partition_mask_status : u32 = 0xC0000000u;\nconst rs_partition_mask_count : u32 = 0x3FFFFFFFu;\nvar<private> kr : array<u32, rs_scatter_block_rows>;\nvar<private> pv : array<u32, rs_scatter_block_rows>;\n\nfn fill_kv_even(wid: u32, lid: u32) {\n    let subgroup_id = lid / histogram_sg_size;\n    let subgroup_invoc_id = lid - subgroup_id * histogram_sg_size;\n    let subgroup_keyvals = rs_scatter_block_rows * histogram_sg_size;\n    let rs_block_keyvals: u32 = rs_histogram_block_rows * histogram_wg_size;\n    let kv_in_offset = wid * rs_block_keyvals + subgroup_id * subgroup_keyvals + subgroup_invoc_id;\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_sg_size;\n        kv[i] = keys[pos];\n    }\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_sg_size;\n        pv[i] = payload_a[pos];\n    }\n}\n\nfn fill_kv_odd(wid: u32, lid: u32) {\n    let subgroup_id = lid / histogram_sg_size;\n    let subgroup_invoc_id = lid - subgroup_id * histogram_sg_size;\n    let subgroup_keyvals = rs_scatter_block_rows * histogram_sg_size;\n    let rs_block_keyvals: u32 = rs_histogram_block_rows * histogram_wg_size;\n    let kv_in_offset = wid * rs_block_keyvals + subgroup_id * subgroup_keyvals + subgroup_invoc_id;\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_sg_size;\n        kv[i] = keys_b[pos];\n    }\n    for (var i = 0u; i < rs_histogram_block_rows; i++) {\n        let pos = kv_in_offset + i * histogram_sg_size;\n        pv[i] = payload_b[pos];\n    }\n}\nfn scatter(pass_: u32, lid: vec3<u32>, gid: vec3<u32>, wid: vec3<u32>, nwg: vec3<u32>, partition_status_invalid: u32, partition_status_reduction: u32, partition_status_prefix: u32) {\n    let partition_mask_invalid = partition_status_invalid << 30u;\n    let partition_mask_reduction = partition_status_reduction << 30u;\n    let partition_mask_prefix = partition_status_prefix << 30u;\n    // kv_filling is done in the scatter_even and scatter_odd functions to account for front and backbuffer switch\n    // in the reference there is a nulling of the smmem here, was moved to line 251 as smem is used in the code until then\n\n    // The following implements conceptually the same as the\n    // Emulate a \"match\" operation with broadcasts for small subgroup sizes (line 665 ff in scatter.glsl)\n    // The difference however is, that instead of using subrgoupBroadcast each thread stores\n    // its current number in the smem at lid.x, and then looks up their neighbouring values of the subgroup\n    let subgroup_id = lid.x / histogram_sg_size;\n    let subgroup_offset = subgroup_id * histogram_sg_size;\n    let subgroup_tid = lid.x - subgroup_offset;\n    let subgroup_count = {scatter_wg_size}u / histogram_sg_size;\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        let u_val = bitcast<u32>(kv[i]);\n        let digit = extractBits(u_val, pass_ * rs_radix_log2, rs_radix_log2);\n        atomicStore(&smem[lid.x], digit);\n        var count = 0u;\n        var rank = 0u;\n        \n        for (var j = 0u; j < histogram_sg_size; j++) {\n            if atomicLoad(&smem[subgroup_offset + j]) == digit {\n                count += 1u;\n                if j <= subgroup_tid {\n                    rank += 1u;\n                }\n            }\n        }\n        \n        kr[i] = (count << 16u) | rank;\n    }\n    \n    zero_smem(lid.x);   // now zeroing the smmem as we are now accumulating the histogram there\n    workgroupBarrier();\n\n    // The final histogram is stored in the smem buffer\n    for (var i = 0u; i < subgroup_count; i++) {\n        if subgroup_id == i {\n            for (var j = 0u; j < rs_scatter_block_rows; j++) {\n                let v = bitcast<u32>(kv[j]);\n                let digit = extractBits(v, pass_ * rs_radix_log2, rs_radix_log2);\n                let prev = histogram_load(digit);\n                let rank = kr[j] & 0xFFFFu;\n                let count = kr[j] >> 16u;\n                kr[j] = prev + rank;\n\n                if rank == count {\n                    histogram_store(digit, (prev + count));\n                }\n                \n                // TODO: check if the barrier here is needed\n            }            \n        }\n        workgroupBarrier();\n    }\n    // kr filling is now done and contains the total offset for each value to be able to \n    // move the values into order without having any collisions\n    \n    // we do not check for single work groups (is currently not assumed to occur very often)\n    let partition_offset = lid.x + partitions_base_offset();    // is correct, the partitions pointer does not change\n    let partition_base = wid.x * rs_radix_size;\n    if wid.x == 0u {\n        // special treating for the first workgroup as the data might be read back by later workgroups\n        // corresponds to rs_first_prefix_store\n        let hist_offset = pass_ * rs_radix_size + lid.x;\n        if lid.x < rs_radix_size {\n            // let exc = histograms[hist_offset];\n            let exc = atomicLoad(&histograms[hist_offset]);\n            let red = histogram_load(lid.x);// scatter_smem[rs_keyval_size + lid.x];\n            \n            scatter_smem[lid.x] = exc;\n            \n            let inc = exc + red;\n\n            atomicStore(&histograms[partition_offset], inc | partition_mask_prefix);\n        }\n    }\n    else {\n        // standard case for the \"inbetween\" workgroups\n        \n        // rs_reduction_store, only for inbetween workgroups\n        if lid.x < rs_radix_size && wid.x < nwg.x - 1u {\n            let red = histogram_load(lid.x);\n            atomicStore(&histograms[partition_offset + partition_base], red | partition_mask_reduction);\n        }\n        \n        // rs_loopback_store\n        if lid.x < rs_radix_size {\n            var partition_base_prev = partition_base - rs_radix_size;\n            var exc                 = 0u;\n\n            // Note: Each workgroup invocation can proceed independently.\n            // Subgroups and workgroups do NOT have to coordinate.\n            while true {\n                //let prev = atomicLoad(&histograms[partition_offset]);// histograms[partition_offset + partition_base_prev];\n                let prev = atomicLoad(&histograms[partition_base_prev + partition_offset]);// histograms[partition_offset + partition_base_prev];\n                if (prev & rs_partition_mask_status) == partition_mask_invalid {\n                    continue;\n                }\n                exc += prev & rs_partition_mask_count;\n                if (prev & rs_partition_mask_status) != partition_mask_prefix {\n                    // continue accumulating reduction\n                    partition_base_prev -= rs_radix_size;\n                    continue;\n                }\n\n                // otherwise save the exclusive scan and atomically transform the\n                // reduction into an inclusive prefix status math: reduction + 1 = prefix\n                scatter_smem[lid.x] = exc;\n\n                if wid.x < nwg.x - 1u { // only store when inbetween, skip for last workgrup\n                    atomicAdd(&histograms[partition_offset + partition_base], exc | (1u << 30u));\n                }\n                break;\n            }\n        }\n    }\n    // special case for last workgroup is also done in the \"inbetween\" case\n    \n    // compute exclusive prefix scan of histogram\n    // corresponds to rs_prefix\n    // TODO make sure that the data is put into smem\n    prefix_reduce_smem(lid.x);\n    workgroupBarrier();\n\n    // convert keyval rank to local index, corresponds to rs_rank_to_local\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        let v = bitcast<u32>(kv[i]);\n        let digit = extractBits(v, pass_ * rs_radix_log2, rs_radix_log2);\n        let exc   = histogram_load(digit);\n        let idx   = exc + kr[i];\n        \n        kr[i] |= (idx << 16u);\n    }\n    workgroupBarrier();\n    \n    // reorder kv[] and kr[], corresponds to rs_reorder\n    let smem_reorder_offset = rs_radix_size;\n    let smem_base = smem_reorder_offset + lid.x;  // as we are in smem, the radix_size offset is not needed\n\n    // keyvalues ----------------------------------------------\n    // store keyval to sorted location\n    for (var j = 0u; j < rs_scatter_block_rows; j++) {\n        let smem_idx = smem_reorder_offset + (kr[j] >> 16u) - 1u;\n        \n        scatter_smem[smem_idx] = bitcast<u32>(kv[j]);\n    }\n    workgroupBarrier();\n\n    // Load keyval dword from sorted location\n    for (var j = 0u; j < rs_scatter_block_rows; j++) {\n        kv[j] = scatter_smem[smem_base + j * {scatter_wg_size}u];\n    }\n    workgroupBarrier();\n    // payload ----------------------------------------------\n    // store payload to sorted location\n    for (var j = 0u; j < rs_scatter_block_rows; j++) {\n        let smem_idx = smem_reorder_offset + (kr[j] >> 16u) - 1u;\n        \n        scatter_smem[smem_idx] = pv[j];\n    }\n    workgroupBarrier();\n\n    // Load payload dword from sorted location\n    for (var j = 0u; j < rs_scatter_block_rows; j++) {\n        pv[j] = scatter_smem[smem_base + j * {scatter_wg_size}u];\n    }\n    workgroupBarrier();\n    \n    // store the digit-index to sorted location\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        let smem_idx = smem_reorder_offset + (kr[i] >> 16u) - 1u;\n        scatter_smem[smem_idx] = kr[i];\n    }\n    workgroupBarrier();\n\n    // Load kr[] from sorted location -- we only need the rank\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        kr[i] = scatter_smem[smem_base + i * {scatter_wg_size}u] & 0xFFFFu;\n    }\n    \n    // convert local index to a global index, corresponds to rs_local_to_global\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        let v = bitcast<u32>(kv[i]);\n        let digit = extractBits(v, pass_ * rs_radix_log2, rs_radix_log2);\n        let exc   = scatter_smem[digit];\n\n        kr[i] += exc - 1u;\n    }\n    \n    // the storing is done in the scatter_even and scatter_odd functions as the front and back buffer changes\n}\n\n@compute @workgroup_size({scatter_wg_size})\nfn scatter_even(@builtin(workgroup_id) wid: vec3<u32>, @builtin(local_invocation_id) lid: vec3<u32>, @builtin(global_invocation_id) gid: vec3<u32>, @builtin(num_workgroups) nwg: vec3<u32>) {\n    if gid.x == 0u {\n        infos.odd_pass = (infos.odd_pass + 1u) % 2u; // for this to work correctly the odd_pass has to start 1\n    }\n    let cur_pass = infos.even_pass * 2u;\n    \n    // load from keys, store to keys_b\n    fill_kv_even(wid.x, lid.x);\n\n    let partition_status_invalid = 0u;\n    let partition_status_reduction = 1u;\n    let partition_status_prefix = 2u;\n    scatter(cur_pass, lid, gid, wid, nwg, partition_status_invalid, partition_status_reduction, partition_status_prefix);\n\n    // store keyvals to their new locations, corresponds to rs_store\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        keys_b[kr[i]] = kv[i];\n    }\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        payload_b[kr[i]] = pv[i];\n    }\n}\n@compute @workgroup_size({scatter_wg_size})\nfn scatter_odd(@builtin(workgroup_id) wid: vec3<u32>, @builtin(local_invocation_id) lid: vec3<u32>, @builtin(global_invocation_id) gid: vec3<u32>, @builtin(num_workgroups) nwg: vec3<u32>) {\n    if gid.x == 0u {\n        infos.even_pass = (infos.even_pass + 1u) % 2u; // for this to work correctly the even_pass has to start at 0\n    }\n    let cur_pass = infos.odd_pass * 2u + 1u;\n\n    // load from keys_b, store to keys\n    fill_kv_odd(wid.x, lid.x);\n\n    let partition_status_invalid = 2u;\n    let partition_status_reduction = 3u;\n    let partition_status_prefix = 0u;\n    scatter(cur_pass, lid, gid, wid, nwg, partition_status_invalid, partition_status_reduction, partition_status_prefix);\n\n    // store keyvals to their new locations, corresponds to rs_store\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        keys[kr[i]] = kv[i];\n    }\n    for (var i = 0u; i < rs_scatter_block_rows; i++) {\n        payload_a[kr[i]] = pv[i];\n    }\n\n    // the indirect buffer is reset after scattering via write buffer, see record_scatter_indirect for details\n}\n";
